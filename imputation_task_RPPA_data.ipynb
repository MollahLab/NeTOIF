{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Author: Min Shi\n",
    "## Last updated: 5/18/2021\n",
    "## Description:\n",
    "The code was created to implement the NetOIF model to impute RPPA data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from models import gcn, lstm\n",
    "from configs import *\n",
    "from utils import *\n",
    "import scipy.sparse\n",
    "import statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It performs the feature imputation task in which all historical features \n",
    "before time t are used to impute features at time t in a supervised manner.\n",
    "The adopted method concatenates GCN and LSTM in a unified framework, where\n",
    "GCN is utilized to model the PPI network and LSTM is utilized to mdoel the \n",
    "time-serial features. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load the RPPA data\n",
    "\n",
    "* **`RPPA_level1.xlsx:`** Reverse-Phase Protein Array, representing the ligand-protein regulations at 1,4,8,24 and 48 hour\n",
    "* **`string_interactions.tsv:`** The protein-protein interaction (PPI) network from stringDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123)\n",
    "FLAGS = tf.flags.FLAGS\n",
    "dataset = FLAGS.rppa\n",
    "time_steps = 5\n",
    "hidden_dim = 6\n",
    "hidden_size = 6\n",
    "train_ratio = FLAGS.train_ratio\n",
    "num_run = FLAGS.num_run\n",
    "train_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_file = os.path.join('datasets/rppa', 'MDD_RPPA_Level3_preprocessed_2020-9.xlsx')\n",
    "feat_df = pd.read_excel(feat_file, sheet_name='MDD_RPPA_Level3_annotated').set_index('Protein')\n",
    "feat_df.columns = [c.split('_')[0]+'_'+c.split('_')[1] for c in feat_df.columns]\n",
    "feat_df = feat_df.loc[:,~feat_df.columns.str.startswith('Ctrl')]\n",
    "feat_df = feat_df.apply(pd.to_numeric, errors='ignore')\n",
    "feat_df = feat_df.groupby(feat_df.columns, axis=1, sort=False).mean()\n",
    "\n",
    "feats = feat_df.values\n",
    "feats = feats.reshape([feats.shape[0],6,5]).transpose([2, 0, 1])\n",
    "feat_list = []\n",
    "for i in range(time_steps):\n",
    "    feat_list.append(feats[i])\n",
    "#         feat_list.append(sp.coo_matrix(feats[i]))\n",
    "\n",
    "#     print(feats.shape)\n",
    "\n",
    "pnames = feat_df.index.tolist()\n",
    "\n",
    "network_file = os.path.join('datasets/rppa', 'string_interactions_new.tsv')\n",
    "net_df = pd.read_csv(network_file,sep='\\t')\n",
    "\n",
    "adj = np.zeros([len(pnames),len(pnames)])\n",
    "links = []\n",
    "no_links = []\n",
    "for index, row in net_df.iterrows():\n",
    "    try:\n",
    "        v1_idx = pnames.index(row['node1'])\n",
    "        v2_idx = pnames.index(row['node2'])\n",
    "        score = float(row['combined_score'])\n",
    "\n",
    "        adj[v1_idx,v2_idx] = score\n",
    "        adj[v2_idx,v1_idx] = score\n",
    "\n",
    "        links.append((row['node1'], row['node2']))\n",
    "    except:\n",
    "        no_links.append((row['node1'], row['node2']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Adjacency matrix:'+str(adj.shape))\n",
    "print('Time series rppa data:'+str(feats.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adjs, feats, train_mask, val_mask, test_mask = load_data_impute(dataset, time_steps, train_ratio)\n",
    "# adjs, feats, train_masks, val_masks, test_masks, protein_names, ligand_names = load_data_impute(dataset, time_steps, \n",
    "#                                                                                                 train_ratio, num_run)\n",
    "adjs, feats, train_masks, val_masks, test_masks, protein_names, ligand_names = load_data_impute(dataset, time_steps, \n",
    "                                                                                                train_ratio, num_run,\n",
    "                                                                                               True, sparse_rate = 0.5)\n",
    "train_mask, val_mask, test_mask = [train_masks[num_run-1], val_masks[num_run-1], test_masks[num_run-1]]\n",
    "\n",
    "num_node = adjs[0].shape[0]\n",
    "num_feat = feats[0].shape[1]\n",
    "for i in range(time_steps):\n",
    "    adjs[i] = sparse_to_tuple(scipy.sparse.coo_matrix(adjs[i]))\n",
    "#     feats[i] = sparse_to_tuple(scipy.sparse.coo_matrix(feats[i]))\n",
    "num_features_nonzeros = [x[1].shape for x in feats]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Prepare the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.1\n",
    "\n",
    "# define placeholders of the input data \n",
    "phs = {\n",
    "        'adjs': [tf.sparse_placeholder(tf.float32, shape=(None, None), name=\"adjs\") for i in\n",
    "             range(time_steps)],\n",
    "        'feats': [tf.placeholder(tf.float32, shape=(None, num_feat), name=\"feats\") for _ in\n",
    "                 range(time_steps)],\n",
    "        'train_mask': tf.placeholder(tf.float32, shape=(None,None), name=\"train_mask\"),\n",
    "        'val_mask': tf.placeholder(tf.float32, shape=(None,None), name=\"val_mask\"),\n",
    "        'test_mask': tf.placeholder(tf.float32, shape=(None,None), name=\"test_mask\"),\n",
    "        'sample_idx': tf.placeholder(tf.int32, shape=(FLAGS.batch_size,), name='batch_sample_idx'),\n",
    "        'dropout_prob': tf.placeholder_with_default(0., shape=()),\n",
    "        'num_features_nonzeros': [tf.placeholder(tf.int64) for i in range(time_steps)]\n",
    "        }\n",
    "\n",
    "# define the GCN model\n",
    "gcn_model = gcn.GraphConvLayer(time_steps = time_steps,\n",
    "                               gcn_layers=FLAGS.gcn_layers,\n",
    "                               input_dim=num_feat,\n",
    "                               hidden_dim=hidden_dim,\n",
    "                               output_dim=hidden_size,\n",
    "                               name='nn_fc1',\n",
    "                               num_features_nonzeros=phs['num_features_nonzeros'],\n",
    "                               act=tf.nn.relu,\n",
    "                               dropout_prob=phs['dropout_prob'],\n",
    "                               dropout=True)\n",
    "\n",
    "embeds_list = gcn_model(adjs=phs['adjs'],\n",
    "                    feats=phs['feats'],\n",
    "                    sparse=False)\n",
    "\n",
    "# prepare train data for the LSTM-based prediction model\n",
    "## replace all missing features at (time_steps-1) with GCN imputed features\n",
    "# embeds_list[time_steps-1] = tf.add(phs['feats'][time_steps-1], \n",
    "#                                    tf.multiply(phs['test_mask'][time_steps-1], embeds_list[time_steps-1]))\n",
    "## construct training samples for the prediction task\n",
    "combined_feats = []\n",
    "for i in range(time_steps):\n",
    "    combined_feats.append(alpha*tf.add(phs['feats'][i], (1-alpha)*embeds_list[i]))\n",
    "\n",
    "x_input, y_label = build_train_samples_imputation(embeds_list=combined_feats, \n",
    "                                                     feats=phs['feats'], \n",
    "                                                     time_steps=time_steps)\n",
    "# define the bi-directional LSTM model\n",
    "lstm_model = lstm.BiLSTM(hidden_size=hidden_size,\n",
    "                         seq_len=FLAGS.time_steps-1,\n",
    "                         holders=phs)\n",
    "x_input_seq = tf.gather(x_input, phs['sample_idx'])\n",
    "y_input_seq_real = tf.gather(y_label, phs['sample_idx'])\n",
    "y_input_seq_mask = tf.gather(phs['train_mask'], phs['sample_idx'])\n",
    "y_input_seq_pred = lstm_model(input_seq=x_input_seq)\n",
    "\n",
    "\n",
    "with tf.name_scope('optimizer'):\n",
    "    # calculate the train mse and ad\n",
    "    train_mse = tf.losses.mean_squared_error(tf.multiply(y_input_seq_real,y_input_seq_mask), \n",
    "                                             tf.multiply(y_input_seq_pred,y_input_seq_mask))\n",
    "    train_absolute_diff = tf.losses.absolute_difference(tf.multiply(y_input_seq_real,y_input_seq_mask), \n",
    "                                             tf.multiply(y_input_seq_pred,y_input_seq_mask))\n",
    "    \n",
    "    # calculate the val mse and ad\n",
    "    val_input_seq_pred = lstm_model(input_seq=x_input)\n",
    "    val_mse = tf.losses.mean_squared_error(tf.multiply(y_label, phs['val_mask']), \n",
    "                                           tf.multiply(val_input_seq_pred, phs['val_mask']))\n",
    "    val_absolute_diff = tf.losses.absolute_difference(tf.multiply(y_label, phs['val_mask']), \n",
    "                                           tf.multiply(val_input_seq_pred, phs['val_mask']))\n",
    "    \n",
    "    # calculate the test mse and ad\n",
    "    test_input_seq_pred = lstm_model(input_seq=x_input)\n",
    "    test_mse = tf.losses.mean_squared_error(tf.multiply(y_label, phs['test_mask']), \n",
    "                                            tf.multiply(test_input_seq_pred, phs['test_mask']))\n",
    "    test_absolute_diff = tf.losses.absolute_difference(tf.multiply(y_label, phs['test_mask']), \n",
    "                                            tf.multiply(test_input_seq_pred, phs['test_mask']))\n",
    "    \n",
    "    missing_actual = tf.multiply(y_label, phs['test_mask'])\n",
    "    missing_predicted = tf.multiply(test_input_seq_pred, phs['test_mask'])\n",
    "    \n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=FLAGS.learning_rate)\n",
    "    opt_op = optimizer.minimize(train_mse)\n",
    "\n",
    "n_cpus = 8\n",
    "config = tf.ConfigProto(device_count={ \"CPU\": n_cpus},\n",
    "                            inter_op_parallelism_threads=n_cpus,\n",
    "                            intra_op_parallelism_threads=2)\n",
    "\n",
    "feed_dict = {phs['train_mask']: train_mask,\n",
    "             phs['val_mask']: val_mask,\n",
    "             phs['test_mask']: test_mask,\n",
    "             phs['sample_idx']: None,\n",
    "             phs['dropout_prob']: FLAGS.dropout_prob}\n",
    "\n",
    "feed_dict.update({phs['adjs'][t]: adjs[t] for t in range(time_steps)})\n",
    "feed_dict.update({phs['feats'][t]: feats[t] for t in range(time_steps)})\n",
    "feed_dict.update({phs['num_features_nonzeros'][t]: num_features_nonzeros[t] for t in range(time_steps)})\n",
    "\n",
    "feed_dict_val = {phs['train_mask']: train_mask,\n",
    "                 phs['val_mask']: val_mask,\n",
    "                 phs['test_mask']: test_mask,\n",
    "                 phs['dropout_prob']: 0.}\n",
    "\n",
    "feed_dict_val.update({phs['adjs'][t]: adjs[t] for t in range(time_steps)})\n",
    "feed_dict_val.update({phs['feats'][t]: feats[t] for t in range(time_steps)})\n",
    "feed_dict_val.update({phs['num_features_nonzeros'][t]: num_features_nonzeros[t] for t in range(time_steps)})\n",
    "\n",
    "def get_batch_idx(epoch):\n",
    "    s = FLAGS.batch_size * epoch\n",
    "    e = FLAGS.batch_size * (epoch + 1)\n",
    "    idx = []\n",
    "    for i in range(s,e):\n",
    "        idx.append(i%num_node)\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. run the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = FLAGS.epochs\n",
    "save_step = 10\n",
    "t = time.time()\n",
    "\n",
    "test_MSEs = []\n",
    "test_ADs = []\n",
    "for k in range(num_run):\n",
    "    train_mask_, val_mask_, test_mask_ = [train_masks[k], val_masks[k], test_masks[k]]\n",
    "    feed_dict.update({phs['train_mask']:train_mask_,\n",
    "                     phs['val_mask']: val_mask_,\n",
    "                     phs['test_mask']: test_mask_})\n",
    "    feed_dict_val.update({phs['train_mask']:train_mask_,\n",
    "                     phs['val_mask']: val_mask_,\n",
    "                     phs['test_mask']: test_mask_})\n",
    "\n",
    "    sess = tf.Session(config=config)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for epoch in range(epochs):\n",
    "        batch_samples = get_batch_idx(epoch)\n",
    "        feed_dict.update({phs['sample_idx']: batch_samples})\n",
    "        _, train_MSE, train_AD = sess.run((opt_op, train_mse, train_absolute_diff), feed_dict=feed_dict)\n",
    "        val_MSE, val_AD = sess.run((val_mse, val_absolute_diff), \n",
    "                                             feed_dict=feed_dict_val) \n",
    "        \n",
    "#         print(\"Epoch:\", '%04d' % (epoch + 1),\n",
    "#           \"train_loss=\", \"{:.5f}\".format(train_MSE),\n",
    "#           \"train_MSE=\", \"{:.5f}\".format(train_MSE),\n",
    "#           \"train_AD=\", \"{:.5f}\".format(train_AD),\n",
    "#           \"val_MSE=\", \"{:.5f}\".format(val_MSE),\n",
    "#           \"val_AD=\", \"{:.5f}\".format(val_AD),\n",
    "#           \"time=\", \"{:.5f}\".format(time.time() - t))\n",
    "        \n",
    "#         if (epoch+1) % save_step == 0:\n",
    "    test_MSE, test_AD, embeds_list_out, missing_actual_, missing_predicted_ = sess.run((test_mse, test_absolute_diff, \n",
    "                                                                   embeds_list, missing_actual, missing_predicted), \n",
    "                                                                   feed_dict=feed_dict_val) \n",
    "    \n",
    "    print(\"run={}-------test_MSE=\".format(k+1), \n",
    "          \"{:.5f}\".format(test_MSE),\n",
    "          \"test_AD=\", \"{:.5f}\".format(test_AD))\n",
    "    \n",
    "    test_MSEs.append(float(test_MSE))\n",
    "    test_ADs.append(float(test_AD))\n",
    "            \n",
    "average_MSE = statistics.mean(test_MSEs)\n",
    "stdev_MSE = statistics.stdev(test_MSEs)\n",
    "average_AD = statistics.mean(test_ADs)\n",
    "stdev_AD = statistics.stdev(test_ADs)\n",
    "print('average_MSE=%f, stdev_MSE=%f, average_AD=%f, stdev_AD=%f' % (average_MSE, stdev_MSE,\n",
    "                                                                      average_AD, stdev_AD))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.ticker as plticker\n",
    "from matplotlib import colors\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "protein_names = np.array(protein_names)\n",
    "\n",
    "fig = plt.figure(figsize=(4,6))\n",
    "fig.subplots_adjust(hspace=0, wspace=0.4)\n",
    "ax1 = fig.add_subplot(121)\n",
    "ax1.set_aspect('auto')\n",
    "cax1 = ax1.matshow(missing_actual_,vmin=-2,vmax=2,cmap='bwr',norm = colors.DivergingNorm(vcenter=0), aspect=\"auto\")\n",
    "fig.colorbar(cax1)\n",
    "ax1.set_title('Actual_48h',y=-0.1, fontsize=11)\n",
    "ax1.set_xticklabels(range(7),fontsize=11)\n",
    "\n",
    "ax1.set_yticks(range(len(protein_names)))\n",
    "loc = plticker.MultipleLocator(base=10) # this locator puts ticks at regular intervals\n",
    "ax1.yaxis.set_major_locator(loc)\n",
    "show_protein_names = protein_names[range(0, len(protein_names), 10)]\n",
    "show_protein_names = np.insert(show_protein_names, 0, 0, axis=0)\n",
    "# ligand_names = np.insert(ligand_names, 0, 0, axis=0)\n",
    "ax1.set_yticklabels(show_protein_names, fontsize=11)\n",
    "ax1.set_xticklabels(ligand_names, fontsize=11, rotation=90)\n",
    "\n",
    "\n",
    "fig1 = plt.figure(figsize=(4,6))\n",
    "fig1.subplots_adjust(hspace=0, wspace=0.4)\n",
    "\n",
    "ax2 = fig1.add_subplot(122)\n",
    "ax2.set_aspect('auto')\n",
    "cax2 = ax2.matshow(missing_predicted_,vmin=-2,vmax=2,cmap='bwr',norm = colors.DivergingNorm(vcenter=0), aspect=\"auto\")\n",
    "fig1.colorbar(cax2)\n",
    "ax2.set_title('Imputed_48h',y=-0.1, fontsize=11)\n",
    "ax2.set_xticklabels(range(7),fontsize=11)\n",
    "\n",
    "ax2.set_yticks(range(len(protein_names)))\n",
    "ax2.yaxis.set_major_locator(loc)\n",
    "ax2.set_yticklabels(show_protein_names, fontsize=11)\n",
    "ax2.set_xticklabels(ligand_names, fontsize=11, rotation=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
